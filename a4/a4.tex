\documentclass[]{article}

\usepackage[]{geometry}
\geometry{
  top=1in,            % <-- you want to adjust this
  inner=1in,
  outer=1in,
  bottom=1in,
  headheight=3ex,       % <-- and this
  headsep=2ex,          % <-- and this
}
\usepackage[T1]{fontenc}
\usepackage{cmbright}
\usepackage{mathtools}
\usepackage{algorithmic}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{parskip}
\usepackage{titling}
\usepackage{fancyvrb}
\pretitle{\begin{flushleft}\LARGE\sffamily}
\posttitle{\par\end{flushleft}\vskip 0.5em}
\preauthor{\begin{flushleft}}
\postauthor{\par\end{flushleft}}
\predate{\begin{flushleft}\scshape}
\postdate{\par\end{flushleft}}
\setlength{\droptitle}{-20pt}
\setlength{\headheight}{15.2pt}
\pagestyle{fancy}

\setcounter{secnumdepth}{1}

\fancyhf{}
\renewcommand{\headrulewidth}{0pt} % remove line at top
\lhead{\fancyplain{}{CS 4780 A4}}
\rhead{\fancyplain{}{Justin Cheng \emph{jc882}, Sunling Selena Yang \emph{sy483}}}
\rfoot{\fancyplain{}{\thepage}}

\begin{document}

\title{CS 4780 Assignment 4}
\author{Justin Cheng \emph{jc882} and Sunling Selena Yang \emph{sy483}}
\date{\today}
\maketitle

\hrule
\vskip 1em

\section{Kernels}

\subsection*{a.}
Modifying the dual perceptron algorithm,

\begin{algorithmic}
\STATE $\alpha_i = 0 ~\forall~ i = 1,...,n$
\FOR{$i=1$ to $n$}
	\IF{$y_i\sum_{j=1}^n \alpha_j K(x_j, x_i)) \le 0$}
		\STATE $\alpha_i = \alpha_i + y_i$
	\ENDIF
\ENDFOR
\STATE Output $\sum_{j=1}^n \alpha_j x_j$
\end{algorithmic}

The prediction rule becomes

$\operatorname{sign}(\sum_{j=1}^n \alpha_j K(x_j, x))$ for some test sample $x$

\subsection*{b.}

\begin{center}
  \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    C & 0.0001 & 0.0005 & 0.001 & 0.005 & 0.01 & 0.05 & 0.1 \\
    \hline
    0 & 100 & 100 & 100 & 100 & 100 & 100 & 100 \\
    1 & 97.71 & 97.71 & 97.71 & 97.96 & 97.96 & 97.71 & 97.46 \\
    2 & 99.22 & 99.22 & 99.22. & 99.22 & 99.22 & 98.96 & 98.96 \\
    3 & 99.48 & 99.22 & 98.96 & 98.96 & 98.96 & 98.96 & 98.96 \\
    4 & 99.20 & 99.20 & 99.20 & 99.46 & 99.73 & 99.46 & 98.93 \\
    5 & 99.74 & 100 & 100 & 100 & 100 & 99.48 & 100 \\
    6 & 99.74 & 100 & 100 & 100 & 100 & 99.48 & 99.22 \\
    7 & 98.93 & 99.20 & 99.20 & 99.20 & 98.93 & 99.20 & 99.20 \\
    8 & 94.75 & 95.28 & 95.80 & 95.54 & 95.54 & 95.54 & 96.06 \\
    9 & 96.83 & 97.88 & 98.41 & 98.15 & 97.88 & 98.15 & 98.15 \\
    \hline
  \end{tabular}
\end{center}

Pick the minimum $C$ with the highest accuracy - giving the softest boundary which doesn't compromise on accuracy.

The accuracy of the test set is $175/1797 = 0.0974$.

\subsection*{c.}
\begin{verbatim}
svm_learn -c 0.005 -t 1 -d 2 ../data/digits/digits0.train ../data/digits/poly/digits0_2_0.005.model
\end{verbatim}
The new accuracy is $181/1797 = 0.101$

\subsection*{d.}
\begin{verbatim}
svm_multiclass_learn -c 0.5 ../data/digits/digits.train ../data/digits/multi/0.5.model
\end{verbatim}

Obtain a new accuracy of $1657/1797 = 0.922$

\section{Generative Models}

\subsection*{a.}
First,
\begin{align*}
h(\vec x) &= \arg \max_{y \in \{+1,-1\}} Pr(Y=y)e^{-\frac{1}{2}(\vec x - \mu_{y})^2} \\
&= \arg \max_{y \in \{+1,-1\}} -\frac{1}{2}(\vec x - \mu_y)^2 + \ln Pr(Y=y)
\end{align*}

Second, note that if \[-\frac{1}{2}(\vec x - \mu_{+1})^2 + \ln Pr(Y=+1) > -\frac{1}{2}(\vec x - \mu_{-1})^2 + \ln Pr(Y=-1)\], then we pick $+1$. So we can simply consider the sign of \[\left(-\frac{1}{2}(\vec x - \mu_{+1})^2 + \ln Pr(Y=+1)\right) - \left( -\frac{1}{2}(\vec x - \mu_{-1})^2 + \ln Pr(Y=-1) \right)\]

which simplifies to \[(\mu_{+1}-\mu_{-1})\vec x + \ln \frac{Pr(Y=+1)}{Pr(Y=-1)} + \frac{1}{2}(\mu_{-1}^2 - \mu_{+1}^2)\]

Thus, $\vec v = \mu_{+1}-\mu_{-1}$ and $b = \ln \frac{Pr(Y=+1)}{Pr(Y=-1)} + \frac{1}{2}(\mu_{-1}^2 - \mu_{+1}^2)$.

\subsection*{b.}

\begin{align*}
h(d) &= \arg \max_{y \in \{+1,-1\}} Pr(Y=y)\prod_{i=1}^lPr(W=w_i|Y=y) \\
&= 
  h(d) = \begin{cases}
    1 & if Pr(Y=1)\prod_{i=1}^lPr(W=w_i|Y=1) > Pr(Y=-1)\prod_{i=1}^lPr(W=w_i|Y=-1) \\
    -1 & otherwise \\
    \end{cases} \\
&= 
  h(d) = \begin{cases}
    1 \quad if \frac{Pr(Y=1)\prod_{i=1}^lPr(W=w_i|Y=1)}{Pr(Y=-1)\prod_{i=1}^lPr(W=w_i|Y=-1)} > 1 \\
    -1 \quad otherwise \\
    \end{cases}  \\
&= 
  h(d) = \begin{cases}
    1 \quad if \ln(Pr(Y=1)) + \sum_{i=1}^lPr(W=w_i|Y=1) - \ln(Pr(Y=-1)) - \sum_{i=1}^lPr(W=w_i|Y=-1) > 0 \\
    -1 \quad otherwise \\
    \end{cases} \\
&= \operatorname{sign}\left(\ln \frac{Pr(Y=1)}{Pr(Y=-1)} + \sum_{i=1}^l\frac{Pr(W=w_i|Y=1)}{Pr(W=w_i|Y=-1)}\right)
\end{align*}

Now, define $\vec x$ so that $x_i$ = 1 if word i is in the document and 0 if not. 

Then, $v_i = \ln \frac{Pr(W=w_i|Y=1)}{Pr(W=w_i|Y=-1)} $.

Finally, $b = \ln \frac{Pr(Y=+1)}{Pr(Y=-1)} $.

\subsection*{c.}


Let $S$ be the random variable whose value we want to predict, and $A$, $B$ be random variables representing the two candidate attributes. Let $\{x,y\}$ be the only possible values $S$ can take, and let $A,B$ be boolean variables, i.e. they take $\{True,False\}$.

Given

$P(S=x)=\frac{5}{6}$, $P(S=y)=\frac{1}{6}$

$P(A=True|S=x) = 0.2$, $P(A=True|S=y) = 0.5$

and that $B$ is just a copy of $A$, so

$P(B=True|S=x) = 0.2$, $P(B=True|S=y) = 0.5$

Then

$P(S=x|A,B=True) = P(S=x|A=True) = \frac{Pr(S=x \cap A=True)}{Pr(A=True)} = \frac{0.2 \cdot \frac{5}{6}}{0.2 \cdot \frac{5}{6} + 0.5 \cdot \frac{1}{6}} = \frac{2}{3}$

$P(S=y|A,B=True) = \frac{1}{3}$

So the Bayes-optimal labeling labels something that has $A,B$ as true as $x$.

But

$P(S=x)P(A=True|S=x)P(B=True|S=x) = \frac{5}{6} \cdot 0.2 \cdot 0.2 = \frac{1}{30}$

$P(S=x)P(A=True|S=x)P(B=True|S=x) = \frac{1}{6} \cdot 0.5 \cdot 0.5 = \frac{1}{24} > \frac{1}{30}$

So the Naive labeling labels something that has $A,B$ as true as $y$.

\section{Naive Bayes Classifier Implementation}
\subsection*{a.}
Using the default value of $C$, the accuracy obtained is $0.968$, with $31451$ correctly classified, and $1036$ incorrectly classified.

\subsubsection{Best $C$}
Using four-fold cross validation on \texttt{arxiv.norm.train}, the best accuracy is obtained when $C=1$ with validation accuracy of $0.969$.

The accuracy on \texttt{arxiv.norm.test} is $0.968$, with $31451$ correct and $1036$ incorrect. There were $291$ false positives and $745$ false negatives.

\subsubsection{Best $C$ for Unnormalized}
Using four-fold cross validation on \texttt{arxiv.train}, the best accuracy is obtained when $C=0.01$ with validation accuracy of $0.965$.

The accuracy on \texttt{arxiv.test} is $0.964$, with $31327$ correct and $1160$ incorrect. There were $286$ false positives and $874$ false negatives.

It does not seem like normalization helps a lot.

\subsection*{b.}

The accuracy obtained is $0.960$, with $31201$ correct. There was $394$ false positive and $892$ false negatives.

\subsection*{c.}

The accuracy obtained is $0.960$, with $31191$ correct. There were $467$ false positives and $829$ false negatives.

As $\frac{c_{10}}{c_{01}}$ increases, the number of false positives increases and the number of false negatives decreases.

\subsection*{d.}
The value being optimized for in cross-validation is to minimize $c_{10} \cdot$ false negatives + false positives.

Here are the (false positive, false negative) values for each fold of the cross-validation.

(66, 167)
(54, 180)
(65, 158)
(61, 168)

The best $C$ obtained is $C=1$.

The accuracy on the test set is $0.968$, with $314151$ correct.

The false positives and false negatives are $291$ and $745$ respectively.

\subsection*{e.}
Naive Bayes Classifiers are a lot easier to implement and are computationally less expensive.

However, Naive Bayes Classifiers assume independence between features and become less accurate when there is high correlation between features. Because Naive Bayes is a linear classifier, cannot solve more complex problems unlike SVMs with kernels.

\end{document}
